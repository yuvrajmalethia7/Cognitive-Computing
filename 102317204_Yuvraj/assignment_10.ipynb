{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36beb22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 - Text Preprocessing\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the paragraph\n",
    "paragraph = (\n",
    "    \"1 Engaging in sports is crucial for students and children, significantly impacting their overall development \"\n",
    "    \"and well-being.2 The physical health benefits include maintaining a healthy weight, enhancing cardiovascular \"\n",
    "    \"health, and developing motor skills.3 Furthermore, sports contribute to mental and emotional strength by \"\n",
    "    \"reducing stress, boosting self-esteem, and fostering teamwork abilities.4 Additionally, participation in sports \"\n",
    "    \"promotes social skills, enhances academic performance, and builds lifelong competencies such as discipline and \"\n",
    "    \"resilience.5 Overall, sports are vital for cultivating well-rounded individuals.\"\n",
    ")\n",
    "\n",
    "# Step 1: Convert to lowercase and remove punctuation\n",
    "paragraph_lower = re.sub(r'[^\\w\\s]', '', paragraph.lower())\n",
    "print(paragraph_lower)\n",
    "\n",
    "# Step 2: Tokenize words and sentences\n",
    "tokens = word_tokenize(paragraph)\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "print(tokens)\n",
    "print(sentences)\n",
    "\n",
    "# Step 3: Split words using regex\n",
    "words_split = re.split(r'\\W+', paragraph)\n",
    "print(words_split)\n",
    "\n",
    "# Step 4: Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in tokens if word not in stop_words]\n",
    "print(filtered_words)\n",
    "\n",
    "# Step 5: Calculate word frequency\n",
    "word_freq = {}\n",
    "for word in filtered_words:\n",
    "    if word in word_freq:\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "print(word_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23150063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 - Stemming and Lemmatization\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Step 1: Find alphabet words\n",
    "word_alphabet = re.findall(r'\\b\\w\\b', paragraph)\n",
    "print(word_alphabet)\n",
    "\n",
    "# Step 2: Filter words without stopwords\n",
    "filtered_words = [word for word in tokens if word not in stop_words]\n",
    "print(filtered_words)\n",
    "\n",
    "# Step 3: Apply stemming and lemmatization\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ps_stemmed = [ps.stem(word) for word in filtered_words]\n",
    "ls_stemmed = [ls.stem(word) for word in filtered_words]\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(ps_stemmed)\n",
    "print(ls_stemmed)\n",
    "print(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 - Vectorization and Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"Great product, works as expected!\",\n",
    "    \"Worst purchase ever, broke after one use.\",\n",
    "    \"Fantastic service and very fast delivery.\"\n",
    "]\n",
    "\n",
    "# Step 1: Bag of Words (BoW)\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(texts)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow.toarray())\n",
    "\n",
    "# Step 2: TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(tfidf.toarray())\n",
    "\n",
    "# Step 3: Create a dictionary of word occurrence in texts\n",
    "top_words = {}\n",
    "for i, text in enumerate(texts):\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in top_words:\n",
    "            top_words[word].append(i)\n",
    "        else:\n",
    "            top_words[word] = [i]\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 - Text Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "text1 = \"Artificial Intelligence (AI) allows machines to mimic human intelligence by interpreting data and making choices. AI is revolutionizing sectors with use in healthcare, finance, and manufacturing.\"\n",
    "text2 = \"Blockchain is a distributed digital record that keeps track of transactions in a secure and open manner. It is perhaps best recognized for enabling cryptocurrencies such as Bitcoin, but it is also used in supply chain management.\"\n",
    "\n",
    "# Step 1: Preprocess and tokenize the texts\n",
    "preprocess_text1 = re.sub(r'[^\\w\\s]', '', text1.lower())\n",
    "preprocess_text2 = re.sub(r'[^\\w\\s]', '', text2.lower())\n",
    "\n",
    "token1 = word_tokenize(preprocess_text1)\n",
    "token2 = word_tokenize(preprocess_text2)\n",
    "\n",
    "print(token1)\n",
    "print(token2)\n",
    "\n",
    "# Step 2: Jaccard Similarity\n",
    "jaccard = len(set(token1).intersection(set(token2))) / len(set(token1).union(set(token2)))\n",
    "print(\"Jaccard Similarity:\", jaccard)\n",
    "\n",
    "# Step 3: Cosine Similarity using sklearn\n",
    "cosine_sim = cosine_similarity([preprocess_text1], [preprocess_text2])\n",
    "print(\"Cosine Similarity:\", cosine_sim[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac557d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 - Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "review = \"The service was excellent and the staff was friendly.\"\n",
    "blob = TextBlob(review)\n",
    "polarity = blob.sentiment.polarity\n",
    "print(\"Polarity:\", polarity)\n",
    "subjectivity = blob.sentiment.subjectivity\n",
    "print(\"Subjectivity:\", subjectivity)\n",
    "\n",
    "# Step 1: Determine sentiment\n",
    "if polarity > 0.1:\n",
    "    sentiment = \"Positive\"\n",
    "    print(\"Sentiment:\", sentiment)\n",
    "elif polarity < -0.1:\n",
    "    sentiment = \"Negative\"\n",
    "    print(\"Sentiment:\", sentiment)\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "    print(\"Sentiment:\", sentiment)\n",
    "\n",
    "# Step 2: WordCloud visualization\n",
    "wordcloud = WordCloud().generate(review)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a4085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6 - Text Generation with LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "text = \"Artificial Intelligence is about making machines that can think and learn like people.\"\n",
    "\n",
    "# Step 1: Tokenize the paragraph\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "input_sequences = []\n",
    "for i in range(1, len(tokens)):\n",
    "    input_sequences.append(tokens[:i+1])\n",
    "\n",
    "max_seq_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
    "\n",
    "# Step 2: Create and train the model\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "y = np.eye(total_words)[y]\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 10, input_length=max_seq_len-1),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=1)\n",
    "\n",
    "# Step 3: Text generation function\n",
    "def generate_text(seed_text, next_words=15):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                seed_text += \" \" + word\n",
    "                break\n",
    "    return seed_text\n",
    "\n",
    "print(generate_text(\"Artificial Intelligence\"))\n",
    "print(generate_text(\"AI\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
